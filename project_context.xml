<?xml version="1.0" ?>
<Project>
  <TreeStructure>
Skin-Segmentation-Dataset-Builder/
â”œâ”€â”€ demo_images
â”‚   â”œâ”€â”€ original_image.jpg
â”‚   â”œâ”€â”€ preview_mask.jpg
â”‚   â”œâ”€â”€ skin_image.jpg
â”‚   â””â”€â”€ training_mask.jpg
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ build_dataset.py
â”œâ”€â”€ io_utils.py
â”œâ”€â”€ overlay_utils.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ skin_detection.py
</TreeStructure>
  <FileContents>
    <File path="build_dataset.py">&quot;&quot;&quot;
Skin Segmentation Dataset Builder

Converts a classification dataset into a segmentation dataset
by automatically extracting skin-only regions and masks.

Author: Michael Panashe Mudimbu
License: MIT
&quot;&quot;&quot;

import os
from tqdm import tqdm
from io_utils import (
    get_class_names,
    get_image_paths,
    prepare_output_dirs,
    save_image,
    save_mask,
    save_classes_txt,
    save_mask_preview
)
from skin_detection import extract_skin
from overlay_utils import overlay_mask_on_image
import numpy as np


DATASET_ORIGINAL = &quot;dataset_original&quot;
DATASET_OUTPUT = &quot;dataset&quot;


def process_split(split: str, class_names: list):
    &quot;&quot;&quot;
    Processes one dataset split (train / valid / test)
    &quot;&quot;&quot;
    image_paths = get_image_paths(DATASET_ORIGINAL, split, class_names)
    class_to_id = {name: idx + 1 for idx, name in enumerate(class_names)}

    for img_path, class_name in tqdm(image_paths, desc=f&quot;Processing {split}&quot;):
        skin_image, binary_mask = extract_skin(img_path)

        if skin_image is None or binary_mask is None:
            continue

        # -----------------------------
        # Convert binary mask to class-labeled mask
        # -----------------------------
        mask = np.zeros_like(binary_mask, dtype=np.uint8)
        mask[binary_mask &gt; 0] = class_to_id[class_name]
        
        # sanity check: ensure mask class IDs are within valid range
        assert mask.max() &lt;= len(class_names), f&quot;Mask ID exceeds number of classes: {mask.max()}&quot;
        # sanity check: ensure mask has no negative values
        assert mask.min() &gt;= 0, f&quot;Mask has negative values: {mask.min()}&quot;

        filename = os.path.basename(img_path)

        # Save skin image
        save_image(
            skin_image,
            DATASET_OUTPUT,
            split,
            filename
        )
        # Save class-labeled mask
        save_mask(
            mask,
            DATASET_OUTPUT,
            split,
            filename
        )
        # Save colorized preview of mask for human inspection
        save_mask_preview(mask, DATASET_OUTPUT, split, filename, class_names)


def main():
    class_names = get_class_names(DATASET_ORIGINAL)

    prepare_output_dirs(DATASET_OUTPUT)
    save_classes_txt(DATASET_OUTPUT, class_names)

    for split in [&quot;train&quot;, &quot;valid&quot;, &quot;test&quot;]:
        process_split(split, class_names)

    print(&quot;Dataset build complete.&quot;)


if __name__ == &quot;__main__&quot;:
    main()
</File>
    <File path="io_utils.py">&quot;&quot;&quot;
I/O utilities for dataset building.
&quot;&quot;&quot;

import os
import cv2
import numpy as np


def get_class_names(dataset_original):
    train_dir = os.path.join(dataset_original, &quot;train&quot;)
    return sorted([
        d for d in os.listdir(train_dir)
        if os.path.isdir(os.path.join(train_dir, d))
    ])


def get_image_paths(dataset_original, split, class_names):
    paths = []

    for cls in class_names:
        cls_dir = os.path.join(dataset_original, split, cls)
        if not os.path.exists(cls_dir):
            continue

        for file in os.listdir(cls_dir):
            if file.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;, &quot;.jpeg&quot;)):
                paths.append((
                    os.path.join(cls_dir, file),
                    cls
                ))

    return paths


def prepare_output_dirs(dataset_output):
    for sub in [&quot;images&quot;, &quot;masks&quot;]:
        for split in [&quot;train&quot;, &quot;valid&quot;, &quot;test&quot;]:
            os.makedirs(
                os.path.join(dataset_output, sub, split),
                exist_ok=True
            )


def save_image(image, dataset_output, split, filename):
    path = os.path.join(dataset_output, &quot;images&quot;, split, filename)
    cv2.imwrite(path, image)


def save_mask(mask, dataset_output, split, filename):
    path = os.path.join(dataset_output, &quot;masks&quot;, split, filename)
    cv2.imwrite(path, mask)


def save_classes_txt(dataset_output, class_names):
    with open(os.path.join(dataset_output, &quot;classes.txt&quot;), &quot;w&quot;) as f:
        for cls in class_names:
            f.write(f&quot;{cls}\n&quot;)


# ------------------------
# Colorize mask for preview
# ------------------------
def colorize_mask(mask, class_names):
    &quot;&quot;&quot;
    Converts integer mask into RGB color image for human inspection.
    mask: H x W, values 0..N
    class_names: list of class names
    &quot;&quot;&quot;
    h, w = mask.shape
    mask_color = np.zeros((h, w, 3), dtype=np.uint8)

    # Assign a unique color per class
    np.random.seed(42)  # for consistent colors
    colors = {0: (0, 0, 0)}  # background = black
    for idx, cls in enumerate(class_names, start=1):
        colors[idx] = tuple(np.random.randint(0, 256, 3).tolist())

    for cls, color in colors.items():
        mask_color[mask == cls] = color

    return mask_color


def save_mask_preview(mask, dataset_output, split, filename, class_names):
    &quot;&quot;&quot;
    Saves colorized mask preview to masks_preview folder.
    &quot;&quot;&quot;
    preview_dir = os.path.join(dataset_output, &quot;masks_preview&quot;, split)
    os.makedirs(preview_dir, exist_ok=True)
    mask_color = colorize_mask(mask, class_names)
    preview_path = os.path.join(preview_dir, filename)
    cv2.imwrite(preview_path, mask_color)
</File>
    <File path="overlay_utils.py">&quot;&quot;&quot;
Utilities for overlaying masks during inference.
&quot;&quot;&quot;

import cv2
import numpy as np


def overlay_mask_on_image(
    original_image,
    mask,
    color=(0, 255, 0),
    alpha=0.4
):
    &quot;&quot;&quot;
    Overlays a segmentation mask on the original image.

    Args:
        original_image (BGR)
        mask (binary 0/255)
        color (BGR)
        alpha (float)

    Returns:
        overlayed_image
    &quot;&quot;&quot;
    overlay = original_image.copy()
    color_mask = np.zeros_like(original_image)
    color_mask[mask &gt; 0] = color

    cv2.addWeighted(
        color_mask,
        alpha,
        overlay,
        1 - alpha,
        0,
        overlay
    )

    return overlay
</File>
    <File path="README.md"># Skin Segmentation Dataset Builder

An automated tool for converting **skin classification datasets** into **background-free skin segmentation datasets**, with a strong focus on **facial skin segmentation**, while still supporting **non-face skin images**.

This project is designed for **rapid prototyping**, **research**, and **production-ready dataset bootstrapping**.

---

## ğŸš€ What Problem Does This Solve?

Most publicly available skin datasets are **classification datasets**, structured like:

```
dataset_original/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ Normal Skin/
â”‚   â”œâ”€â”€ Acne/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ valid/
â””â”€â”€ test/
```

These datasets are excellent for classification, but **not usable for segmentation tasks** without manual annotation.

Segmentation models require:

* Pixel-level masks
* A different dataset structure
* Careful handling of background pixels

### The Core Challenge

In skin segmentation:

* Background pixels dominate images
* Background becomes an overpowering class
* This introduces noise, imbalance, and poor generalization
* Even cropped face images still contain:

  * Hair
  * Eyes
  * Nostrils
  * Clothing
  * Background artifacts

### This Tool Solves That By:

âœ… Automatically detecting **skin regions only**
âœ… Producing **standalone skin images**
âœ… Producing **standalone skin masks**
âœ… **Removing background entirely** (no background class)
âœ… Preserving **train / valid / test splits**
âœ… Working with:

* Face images (selfies)
* Partial skin images (arms, cheeks, neck, forehead)
* Skin-only datasets

---

## ğŸ§  Key Idea

Instead of labeling background pixels as a class, this tool:

&gt; **Removes all non-skin pixels altogether**

This results in:

* Cleaner segmentation datasets
* No background domination
* Better class balance
* Faster convergence during training

---

## ğŸ§© How It Works (High Level)

For each image:

1. Attempt **face detection** (optional)
2. If a face is detected:

   * Prefer face crop (less noise)
3. If no face is detected:

   * Process the full image
4. Detect **skin pixels only**
5. Generate:

   * Skin-only image
   * Binary skin mask
6. Save outputs in segmentation dataset format
7. Generate **colorized mask previews** for human inspection (optional)

Face detection is **optional and non-blocking**.

---

## ğŸ›  Tools &amp; Technologies Used

* **Python**
* **OpenCV** â€“ image processing
* **MediaPipe** â€“ optional face detection
* **HSV + YCrCb color space filtering** â€“ skin detection
* **Morphological operations** â€“ mask cleanup

No pretrained segmentation model is required.

---

## ğŸ“‚ Input Dataset Format (Required)

The tool expects a **classification dataset** structured as follows:

```
dataset_original/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ Normal Skin/
â”‚   â”œâ”€â”€ Acne/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ valid/
â”‚   â”œâ”€â”€ Normal Skin/
â”‚   â”œâ”€â”€ Acne/
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â”œâ”€â”€ Normal Skin/
    â”œâ”€â”€ Acne/
    â””â”€â”€ ...
```

---

## ğŸ“¦ Output Dataset Format (Generated)

```
dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ valid/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ masks/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ valid/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ masks_preview/      # colorized human-readable masks
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ valid/
â”‚   â””â”€â”€ test/
â””â”€â”€ classes.txt
```

* `masks_preview` contains **colorized masks for visualization** only; **integer masks** in `masks` are used for training.

## ğŸ–¼ Example Output (Single Sample)

Below is a visual example showing how a single image is transformed by the pipeline.

&lt;table&gt;
   &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;b&gt;Original Image&lt;/b&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;b&gt;Skin-only Image&lt;/b&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;b&gt;Training Mask&lt;/b&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;b&gt;Preview Mask&lt;/b&gt;&lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
      &lt;td&gt;
         &lt;img src=&quot;demo_images/original_image.jpg&quot; width=&quot;250&quot;/&gt;
      &lt;/td&gt;
      &lt;td&gt;
         &lt;img src=&quot;demo_images/skin_image.jpg&quot; width=&quot;250&quot;/&gt;
      &lt;/td&gt;
      &lt;td&gt;
         &lt;img src=&quot;demo_images/training_mask.jpg&quot; width=&quot;250&quot;/&gt;
      &lt;/td&gt;
      &lt;td&gt;
         &lt;img src=&quot;demo_images/preview_mask.jpg&quot; width=&quot;250&quot;/&gt;
      &lt;/td&gt;
   &lt;/tr&gt;
&lt;/table&gt;

### Why the Training Mask Looks Almost Black

The **training mask** is stored as a single-channel image where each pixel value represents a **class ID**, not a color.

For example:
- `0` â†’ background (ignored during training)
- `1` â†’ dry skin
- `2` â†’ normal skin
- `3` â†’ oily skin
- ...

Since these values are small integers, they appear **very dark or almost invisible** when viewed as a normal image.

This is intentional.

Segmentation models require **integer-valued masks**, not RGB images.  
The colorized **preview mask** is generated purely for **human inspection** and is **never used during training**.

### Preview Mask Legend

The preview mask uses a fixed color mapping to visualize different skin classes:

- Each color represents a unique skin type or condition
- Colors are assigned automatically and consistently
- Preview masks exist only for debugging and quality checks

---

## ğŸ”§ Installation

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

```bash
python build_dataset.py
```

* The tool will automatically process `train`, `valid`, and `test` splits
* Colorized mask previews will be generated in `dataset/masks_preview`
* Use **CrossEntropyLoss(ignore_index=0)** in PyTorch or equivalent in other frameworks

  * Ensures background pixels are ignored during training
  * Only actual skin pixels contribute to learning

---

## ğŸ“œ License

MIT License

---

## ğŸ“¬ Contact &amp; Support

**Michael Panashe Mudimbu**
ğŸ“§ Email: **[michaelmudimbu@gmail.com](mailto:michaelmudimbu@gmail.com)**
</File>
    <File path="requirements.txt"/>
    <File path="skin_detection.py">&quot;&quot;&quot;
Skin detection and extraction logic.
&quot;&quot;&quot;

import cv2
import numpy as np
import mediapipe as mp
import os

# Suppress TensorFlow logging caused by MediaPipe
os.environ[&quot;TF_CPP_MIN_LOG_LEVEL&quot;] = &quot;2&quot;

mp_face = mp.solutions.face_detection


def detect_face(image):
    &quot;&quot;&quot;
    Detects the primary face in an image.
    Returns a valid face crop or None.
    &quot;&quot;&quot;

    h, w, _ = image.shape

    with mp_face.FaceDetection(
        model_selection=1,
        min_detection_confidence=0.5
    ) as detector:
        results = detector.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

        if not results.detections:
            return None

        bbox = results.detections[0].location_data.relative_bounding_box

        x1 = int(bbox.xmin * w)
        y1 = int(bbox.ymin * h)
        x2 = int((bbox.xmin + bbox.width) * w)
        y2 = int((bbox.ymin + bbox.height) * h)

        # -------------------------
        # CLAMP TO IMAGE BOUNDS
        # -------------------------
        x1 = max(0, x1)
        y1 = max(0, y1)
        x2 = min(w, x2)
        y2 = min(h, y2)

        # -------------------------
        # VALIDATE CROP
        # -------------------------
        if x2 &lt;= x1 or y2 &lt;= y1:
            return None

        face = image[y1:y2, x1:x2]

        if face.size == 0:
            return None

        return face


def skin_color_mask(face_img):
    &quot;&quot;&quot;
    Generates a binary skin mask using HSV + YCrCb thresholds.
    &quot;&quot;&quot;
    hsv = cv2.cvtColor(face_img, cv2.COLOR_BGR2HSV)
    ycrcb = cv2.cvtColor(face_img, cv2.COLOR_BGR2YCrCb)

    hsv_mask = cv2.inRange(
        hsv,
        (0, 40, 60),
        (25, 255, 255)
    )

    ycrcb_mask = cv2.inRange(
        ycrcb,
        (0, 135, 85),
        (255, 180, 135)
    )

    mask = cv2.bitwise_and(hsv_mask, ycrcb_mask)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)

    return mask


def extract_skin(image_path):
    &quot;&quot;&quot;
    Extracts skin-only image and mask.
    Works for:
    - Face images (selfies)
    - Partial skin images
    - Skin-only datasets
    &quot;&quot;&quot;
    # Configuration ------------------------------
    USE_FACE_DETECTION = True
    FACE_REQUIRED = False
    KEEP_ONLY_LARGEST_SKIN_COMPONENT = True
    MAX_SKIN_CONFIDENCE_FILTERING = False
    MAX_SKIN_CONFIDENCE_RATIO = 0.90
    # --------------------------------------------

    # Read image
    image = cv2.imread(image_path)
    if image is None:
        return None, None

    # Try face detection first
    face = detect_face(image)

    if (face is not None) and USE_FACE_DETECTION:
        region = face
    else:
        if FACE_REQUIRED:
            return None, None
        # Fallback to full image
        region = image
    
    # Reject if no skin detected
    if region is None or region.size == 0:
        return None, None

    # Generate skin mask
    mask = skin_color_mask(region)

    # -------------------------------
    # KEEP ONLY LARGEST SKIN COMPONENT
    # -------------------------------
    if KEEP_ONLY_LARGEST_SKIN_COMPONENT:
        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(
            mask, connectivity=8
        )

        # If only background detected
        if num_labels &lt;= 1:
            return None, None

        # Ignore background label 0
        largest_label = 1 + stats[1:, cv2.CC_STAT_AREA].argmax()
        mask = np.uint8(labels == largest_label) * 255
    # -------------------------------

    # Reject images with insufficient skin pixels
    skin_pixel_ratio = np.sum(mask &gt; 0) / mask.size
    if skin_pixel_ratio &lt; 0.05:
        return None, None
    # Reject images with too many skin pixels
    if MAX_SKIN_CONFIDENCE_FILTERING:
        if skin_pixel_ratio &gt; MAX_SKIN_CONFIDENCE_RATIO:
            # Probably overexposed / false positive
            return None, None

    skin_only = cv2.bitwise_and(region, region, mask=mask)

    return skin_only, mask
</File>
  </FileContents>
</Project>
